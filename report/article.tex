\author{Andrea Jemmett, Chiel Kooijman}
\title{Evolving Behaviour for Thymio Robots using NEAT}
% XXX: Include interest-invoking words as real-time, on-line, etc.
\documentclass{article}
\usepackage{graphicx,amsmath}
\usepackage[defaultmono]{droidmono}
\usepackage[T1]{fontenc}
\usepackage{microtype}

\begin{document}
	\maketitle
	\begin{abstract}
		We made robots do cool things.
		% FIXME: Make the abstract less abstract.
	\end{abstract}

	\section{Introduction} % {{{
	\label{sec:Introduction}
	In the field of Evolutionary Robotics (ER), several papers have been
	published over the last decades, but most were either concerned
	with evolving a robot controller in a virtual environment
	\cite{silva2015case, simoes1999evolutionary, sims1994evolving}, or more
	recently about evolving neural networks for behaviour on real robots, but
	only succesfully for limited tasks \cite{heinermanevolution,
	watson1999embodied}. In this paper we explore how more complex behaviour
	can be evolved on real robots using (Hyper?)NEAT \cite{stanley2002evolving}
	and using the right processing of input data. As far as the authors are
	aware this is the first time NEAT has been used to evolve behaviour on
	physical robots.
	% TODO: Something about energy and taking inspiration from life and RL?
	% TODO: and we show how we do impossibly amazing things and are much better
	% at everything than everyone else ever.
	In the next section we cover previous work on ER and NEAT. In section
	\ref{sec:method} we give a precise description on the hardware used, we
	explain the Obstacle Avoidance and Foraging tasks, including the reward
	model, the envorinment in which the robots were evaluated, and the
	algorithm and settings used for the experiments.
	In sections \ref{sec:setup} and \ref{sec:results} we cover the exact
	parameters used to run the experiments and their results respectively.
	Conclusions are drawn and further research suggested in the last section.
	% section Introduction }}}

	\section{Related Work} % {{{
	\label{sec:Related Work}
	\cite{heinermanevolution}
	\cite{silva2012odneat}
	\cite{stanley2002evolving, stanley2009hypercube, stanley2006real}
	% section Related Work }}}

	\section{System Description} % {{{
	\label{sec:method}
	\subsection{Robot} % {{{
	\label{sub:Robot}
	The Thymio II robot includes 7 Infra-Red (IR) proximity sensors able detect
	to obstacles of which are 5 in the front and two in the back (values
	between 0 and around 4500, where a higher value corresponds to a near
	obstacle).
	In the foraging task the robot is outfitted with a Raspberry Pi Camera
	Module, which streams 320x240 pixel images, and a piece of velcro with
	which it can grab the pucks. The camera is placed so that it can still see
	the puck when it has stuck to it while seeing as far forward as possible.
	The robot can move through two differential wheels, meaning that two
	different speeds (range between -300 and 300 for obstacle avoidance and
	-150 to 150 for foraging) can be set for each wheel. For the purpose of our
	research, we extend the standard setup with a more powerful logic board,
	wireless communication, and a high capacity battery. We use a Raspberry Pi
	B+ (credit card-sized single-board computer developed in the UK by the
	Raspberry Pi Foundation) that interacts with the Thymio sensors and
	actuators. A WiFi dongle (Edimax 150Mbps Wireless 802.11b/g/n nano USB WiFi
	Adapter model EW-7811Un) attached to the Raspberry Pi ensures communication
	between the robots. The power is given by a Verbatim Dual USB 12000 mAh
	battery that allows for a total experimental time of 10 hours.
	% FIXME maybe not copy this verbatim?
	% subsection Robot }}}

	\subsection{Environment} % {{{
	\label{sub:Environment}
	The robots operate in an arena of two by two meters with outer walls that
	act as obstacles to avoid.
	In the foraging task one of the corners of the arena is blue with white
	patches to counteract white-balance issues, and there are five green pucks
	with hook and loop fasteners sides placed in the arena.
	% subsection Environment }}}

	\subsection{Obstacle Avoidance} % {{{
	\label{sub:Obstacle Avoidance}
	In the Obstacle Avoidance task a single robot is placed in the
	2x2m environment and has to learn how to avoid its outer walls while moving
	as much as possible. Each robot is evaluated for a fixed period of time $T$
	and its performance is measured using a fitness function.
	\paragraph{Inputs} %Â {{{
	\label{par:obstacle_inputs}
	The input vector to the Neural Network controller is composed by some of
	the proximity IR sensors. We use three sensor in the front (the outermost
	and the central one) and the two sensors from the back. Each value is
	divided by the maximum value for that sensor. The input vector is
	then augmented with a bias value of $1$.
	% paragraph obstacle_inputs }}}

	\paragraph{Fitness function} % {{{
	We use the following performance measure, as in \cite{heinermanevolution}:
	$$f = \sum_{t=0}^{T}{s_{trans} (1 - s_{rot}) (1 - v_{sens})}$$
	where:
	\begin{itemize}
		\item $s_{trans}$ is the translational speed (non normalized)
			computed as the sum of the right motor speed and the left one;
		\item $s_{rot}$ is the rotational speed, computed as the absolute
			difference between the speed values of the two motors and normalized
			in the range $[0, 1]$;
		\item $v_{sens}$ is the value of the proximity sensor closest to an
			obstacle normalized between 0 and 1.
	\end{itemize}
	The aim of this fitness function is to have a robot capable of going as much
	as possible in a straight path (hence the use of $s_{rot}$), with the
	highest possible speed (hence $s_{trans}$) and without going into walls. The
	component $v_{sens}$ is necessary: suppose that the robot is travelling
	along a straight path at maximum velocity against a wall; without the
	$v_{sens}$ component the robot's performance in this case would be maximal.
	By using $v_{sens}$ we prevent this situation and the fitness score will be
	low.
	% paragraph obstacle_fitness }}}
	% subsection Obstacle Avoidance }}}

	\subsection{Foraging} % {{{
	\label{sub:Foraging}
	In the foraging task the goal is to bring pucks to the goal area using the
	camera as often as possible within its evaluation time.

	\paragraph{Inputs} % {{{
	\label{par:foraging_inputs}
	Inputs consist of the distance and angle from the bottom centre of the image
	to the closest pixel that shows the puck and the goal, and a binary flag
	representing whether the robot has the puck. We first apply a Gaussian filter
	to the camera image, then we convert it from \textit{BGR} to \textit{HSV}
	colour space and we apply a thresholding operator to get two binary images:
	one for the pucks and the other for the goal. The puck distance and angle is
	then the distance from the bottom centre of the image to the closest point
	in the binary image. In a similar manner we compute distance and angle of
	the goal. For performance reasons those values have been precomputed and
	stored in two matrices, one for distances and one for angles.
	The evolved Neural Network is recurrent, meaning that a node is not limited
	to be linked to a node in the next layer (as in feed-forward NN) but can be
	linked also to a node in the previous or same layer.
	% paragraph foraging_inputs }}}

	\paragraph{Fitness function} %{{{
	\label{par:foraging_fitness}
	Each robot has a virtual energy level (which it is not aware of). At the
	beginning of the evaluation, the energy level is set to \texttt{INITIAL\_ENERGY}.
	Then, at each successive time step an energy delta is computed and
	subtracted from the energy level. The energy gets decremented by a quantity
	\texttt{ENERGY\_DECAY} at every time step and the following formula holds:
	$$
	\Delta e(t)=
	\begin{cases}
		\mathtt{GOAL\_REACHED\_BONUS} & \textrm{if } goal\_reached(t) \\
		s_{penalty} + \mathtt{GOAL\_BONUS\_SCALE} (d_g(t) - d_g(t-1)) &
		\textrm{if } d_p(t) = 0 \\
		s_{penalty} + \mathtt{PUCK\_BONUS\_SCALE} (d_p(t) - d_p(t-1)) &
		\textrm{otherwise}
	\end{cases}
	$$
	where \texttt{GOAL\_BONUS\_SCALE}, \texttt{PUCK\_BONUS\_SCALE} and
	\texttt{GOAL\_REACHED\_BONUS} are constants, $d_p$ is the distance of the
	puck, $d_g$ is the distance of the goal, $goal\_reached$ is a proposition
	that evaluates to true if the robot has brought a puck to the goal and
	$s_{penalty}$ is a penalty component for going backwards, that is:
	$$
	s_{penalty}=
	\begin{cases}
		\frac{\mathtt{BACKWARD\_PUNISH\_SCALE}}{s_{max}} \sqrt{s_{left} s_{right}}
		  & \textrm{if } s_{left} < 0 \wedge s_{right} < 0 \\
		0 & \textrm{otherwise}
	\end{cases}
	$$
	where $s_{right}$ and $s_{left}$ are right and left motors' speed
	respectively and $s_{max}$ is the maximum allowed speed. We use the
	following proposition to check whether the robot has successfully brought a puck
	to the goal:
	$$
	goal\_reached(t)=
	\begin{cases}
		true & \textrm{if } d_g(t) \le \mathtt{MIN\_GOAL\_DIST} \wedge d_p(t) =	0 \\
		false & \textrm{otherwise}
	\end{cases}
	$$
	% paragraph foraging_fitness }}}

	% xXX image: picture of robot + lego contraption
	% subsection Foraging }}}

	% section System Description }}}

	\section{Experimental Setup} % {{{
	\label{sec:setup}
	In this section we show the values that we use in our experiments for the
	different constants and parameters that we introduced until now.

	\subsection{Obstacle Avoidance}
\begin{table}
	\centering
	\begin{tabular}{l l}
		\texttt{ACTIVATION\_FUNC} & \textbf{tanh} \\
		Neural Network activation functions & \\
		\texttt{POPSIZE} & \textbf{10} \\
		Population size
	\end{tabular}
	\caption{Experimental Setup}
	\label{tab:obstacle_parameters}
\end{table}

	\subsection{Foraging}

	% section Experimental Setup }}}

	\section{Experimental Results} % {{{
	\label{sec:results}
	
	% section results }}}

	\section{Conclusion, Discussion, and Further Work} % {{{
	\label{sec:conclusion}
	Using energy helps limit evaluation time which makes evolving behaviour on
	physical robots more viable.
	% TODO: use experiments to show that this is actually true
	Calculating something akin to distance and angle is possible in real time.
	Limiting the number of input nodes makes learning faster. % TODO does it?

	Due to the time it takes to process the camera input, we had to decrease
	the locomotion speed of the robot so that it had enough time to stop
	turning once it saw the goal or the puck. With a faster processor or
	dedicated GPU image processing time could be decreased, thus allowing us to
	increase the maximum motor speeds, allowing us to evaluate the individuals
	more quickly.
	Furthermore using more robots in a distributed way such as
	odNEAT \cite{silva2012odneat} would decrease the time needed to evolve
	robots able to evolve fitter robots faster.

	Alternatively, Reinforcement Learning (RL) methods such as DQN
	\cite{mnih2013playing} may be more suitable for this type of task, as they
	allow the behaviour to be adapted at every time-step in a way that is more
	likely to result in better fitness than random mutations.
	% section conclusion }}}

	\appendix
	\section{Manual} % {{{
	\label{apx:sec:manual}
	\subsection{Installing} % {{{
	\label{sub:installing}
	When cloning the repository make sure to add the \texttt{{-}{-}recursive}
	flag to include the peas submodule for NEAT.
	For the Obstacle Avoidance task performance can be improved by using
	calibrated sensors, described in \ref{apx:sec:calibration}. How to install
	the visualisation server for Foraging is described in
	\ref{apx:sub:install_server}.
	% subsection installing }}}

	\subsection{Running} % {{{
	\label{apx:sub:running}
	Before running an experiment, make sure that the robots are on and the IP
	addresses are in \texttt{bots.txt}.
	For running the Obstacle Avoidance task do:
	\begin{verbatim}
		./start_all.sh obstacle_avoidance.py
	\end{verbatim}
	For running the Foraging task do:
	\begin{verbatim}
		./start_all.sh foraging.py
	\end{verbatim}
	% subsection running }}}
	% section manual }}}

	\section{Calibration} % {{{
	\label{apx:sec:calibration}
	Since the maximum values differ greatly between different robots and
	sensors, \texttt{parameters.py} contains a dictionary \texttt{SENSORS\_MAX}
	that defines the calibration for each sensor and each robot IP what the
	maxima are.
	% section Calibration }}}

	\section{Real-time Visualisation} % {{{
	\label{apx:sec:visualisation}
	In the foraging task it can be useful to know what the robot perceives. For
	this reason a script was added to see the camera image after processing
	through the web browser. It shows this for each robot in bots.txt. Usage:
	\begin{verbatim}
	$ ./img-stream.py
	hosting server on http://localhost:8080

	# ./img-stream.py
	hosting server on http://localhost
	\end{verbatim}
	% FIXME: Add a screenshot
	\subsection{Installing} % {{{
	\label{apx:sub:install_server}
	For the server to function properly Bootstrap is required. Install this
	by running \texttt{./install\_server.sh} with a working internet
	connection.
	% subsection Known Issues }}}
	% section Real-time Visualisation }}}

	\section{Vision Optimisations} % {{{
	\label{apx:sec:vision_optimisations}
	To increase the processing speed of the camera vision, two matrices are
	precomputed that contain the distance and angle respectively to the centre
	bottom of the image for every pixel.
	These matrices are stored in \texttt{distances.p}. This file is
	automatically generated with \texttt{dist\_angle\_matrices.py} when
	\texttt{start\_all.sh} is ran for the first time.
	% section Vision Optimisations }}}

	\section{Logging} % {{{
	\label{apx:sec:logging}
	The robots locally log a lot of information about each experiment while
	they run. This can be downloaded to\\\texttt{logs/<experiment
	name>\_<timestamp>\_<git hash>.json} by running \texttt{./get\_logs.sh}.

	The \texttt{parse\_log.py} script prints an overview of the mean, maximum,
	and minimum fitness for each generation.

	\texttt{plot\_json.py} shows a simple visualisation of the same
	information.

	% TODO: Describe exactly(?) what is logged.
	% section logging }}}

\bibliographystyle{abbrv}
\bibliography{references}
\end{document}
